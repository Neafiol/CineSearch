{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_discr_ganre_arr_word2int\",\"r\") as f:\n",
    "    dataset=json.load(f)\n",
    "with open(\"gange\",\"r\") as f:\n",
    "    gange = json.load(f)\n",
    "with open(\"vocab\",\"r\") as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextGanre(nn.Module):\n",
    "    def __init__(self,batch_size, vocab_size, embedding_dim, num_class, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.basic_rnn = nn.LSTM(embedding_dim, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.clasifiter = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_class),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # (num_layers, batch_size, n_neurons)\n",
    "        self.c_0 = torch.zeros(1,self.batch_size, self.hidden_size)\n",
    "        self.h_0 = torch.zeros(1,self.batch_size, self.hidden_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (self.h_0 , self.c_0) = self.basic_rnn(embedded, (self.h_0 , self.c_0))\n",
    "        \n",
    "        output = self.clasifiter(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 16\n",
    "NUN_CLASS = 1 #len(gange)\n",
    "NGRAMS = 2\n",
    "BATCH_SIZE = 16\n",
    "VOCAB_SIZE=len(vocab)\n",
    "N_EPOCHS =5 \n",
    "\n",
    "model = TextGanre(BATCH_SIZE, VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    d=[b[\"ganre\"][0] for b in batch]\n",
    "    label = torch.tensor(d).to(torch.float)\n",
    "    text = [torch.tensor(b[\"discr\"]).to(torch.int64) for b in batch]\n",
    "    \n",
    "    text=torch.nn.utils.rnn.pad_sequence( text , batch_first=True)\n",
    "    return text.view(-1,len(batch)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acur(pred, result):\n",
    "\n",
    "    result=result.tolist()\n",
    "    s = 0\n",
    "    c = 1\n",
    "    \n",
    "    for i,p in enumerate(pred[-1]):    \n",
    "        p=p.tolist()\n",
    "        count = sum(result[i])\n",
    "        \n",
    "        top = sorted(p)[-int(count):]\n",
    "        for t in top:\n",
    "            s+=result[i][p.index(t)]\n",
    "        c+=count\n",
    "            \n",
    "    return s/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()        \n",
    "        model.init_hidden() \n",
    "        \n",
    "        text, cls = text.to(device), cls.to(device)\n",
    "        try:\n",
    "            output = model(text)[-1,:,0]\n",
    "\n",
    "            loss = criterion(output, cls)\n",
    "            train_loss += loss.detach().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc += abs((output-cls).mean())\n",
    "\n",
    "            if i%50==1:\n",
    "                print(\"train_acc\",train_acc/i  )\n",
    "        except:\n",
    "            print(\"Error\")\n",
    "            break\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_len = int(len(dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = random_split(dataset, [train_len, len(dataset) - train_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "Epoch: 1  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0000(train)\t|\tAcc: 0.0%(train)\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "Epoch: 2  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0000(train)\t|\tAcc: 0.0%(train)\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "Epoch: 3  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0000(train)\t|\tAcc: 0.0%(train)\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "Epoch: 4  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0000(train)\t|\tAcc: 0.0%(train)\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "Epoch: 5  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0000(train)\t|\tAcc: 0.0%(train)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "#     valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    \n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "#     print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Porter:\n",
    "\tPERFECTIVEGROUND =  re.compile(u\"((ив|ивши|ившись|ыв|ывши|ывшись)|((?<=[ая])(в|вши|вшись)))$\")\n",
    "\tREFLEXIVE = re.compile(u\"(с[яь])$\")\n",
    "\tADJECTIVE = re.compile(u\"(ее|ие|ые|ое|ими|ыми|ей|ий|ый|ой|ем|им|ым|ом|его|ого|ему|ому|их|ых|ую|юю|ая|яя|ою|ею)$\")\n",
    "\tPARTICIPLE = re.compile(u\"((ивш|ывш|ующ)|((?<=[ая])(ем|нн|вш|ющ|щ)))$\")\n",
    "\tVERB = re.compile(u\"((ила|ыла|ена|ейте|уйте|ите|или|ыли|ей|уй|ил|ыл|им|ым|ен|ило|ыло|ено|ят|ует|уют|ит|ыт|ены|ить|ыть|ишь|ую|ю)|((?<=[ая])(ла|на|ете|йте|ли|й|л|ем|н|ло|но|ет|ют|ны|ть|ешь|нно)))$\")\n",
    "\tNOUN = re.compile(u\"(а|ев|ов|ие|ье|е|иями|ями|ами|еи|ии|и|ией|ей|ой|ий|й|иям|ям|ием|ем|ам|ом|о|у|ах|иях|ях|ы|ь|ию|ью|ю|ия|ья|я)$\")\n",
    "\tRVRE = re.compile(u\"^(.*?[аеиоуыэюя])(.*)$\")\n",
    "\tDERIVATIONAL = re.compile(u\".*[^аеиоуыэюя]+[аеиоуыэюя].*ость?$\")\n",
    "\tDER = re.compile(u\"ость?$\")\n",
    "\tSUPERLATIVE = re.compile(u\"(ейше|ейш)$\")\n",
    "\tI = re.compile(u\"и$\")\n",
    "\tP = re.compile(u\"ь$\")\n",
    "\tNN = re.compile(u\"нн$\")\n",
    "\n",
    "\tdef stem(word):\n",
    "\t\tword = word.lower()\n",
    "\t\tword = word.replace(u'ё', u'е')\n",
    "\t\tm = re.match(Porter.RVRE, word)\n",
    "\t\tif m.groups():\n",
    "\t\t\tpre = m.group(1)\n",
    "\t\t\trv = m.group(2)\n",
    "\t\t\ttemp = Porter.PERFECTIVEGROUND.sub('', rv, 1)\n",
    "\t\t\tif temp == rv:\n",
    "\t\t\t\trv = Porter.REFLEXIVE.sub('', rv, 1)\n",
    "\t\t\t\ttemp = Porter.ADJECTIVE.sub('', rv, 1)\n",
    "\t\t\t\tif temp != rv:\n",
    "\t\t\t\t\trv = temp\n",
    "\t\t\t\t\trv = Porter.PARTICIPLE.sub('', rv, 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttemp = Porter.VERB.sub('', rv, 1)\n",
    "\t\t\t\t\tif temp == rv:\n",
    "\t\t\t\t\t\trv = Porter.NOUN.sub('', rv, 1)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\trv = temp\n",
    "\t\t\telse:\n",
    "\t\t\t\trv = temp\n",
    "\t\t\t\n",
    "\t\t\trv = Porter.I.sub('', rv, 1)\n",
    "\n",
    "\t\t\tif re.match(Porter.DERIVATIONAL, rv):\n",
    "\t\t\t\trv = Porter.DER.sub('', rv, 1)\n",
    "\n",
    "\t\t\ttemp = Porter.P.sub('', rv, 1)\n",
    "\t\t\tif temp == rv:\n",
    "\t\t\t\trv = Porter.SUPERLATIVE.sub('', rv, 1)\n",
    "\t\t\t\trv = Porter.NN.sub(u'н', rv, 1)\n",
    "\t\t\telse:\n",
    "\t\t\t\trv = temp\n",
    "\t\t\tword = pre+rv\n",
    "\t\treturn word\n",
    "\tstem=staticmethod(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preproc(text):\n",
    "    reg = re.compile('[^а-яА-Я ]')\n",
    "    text = (reg.sub('', text))\n",
    "    text=re.compile(\"\\s+\").sub(\" \",text).lower()\n",
    "    \n",
    "    res=[]\n",
    "    for w in text.split(\" \"):\n",
    "        \n",
    "        if len(w)>4:\n",
    "            try:\n",
    "                res.append(vocab[Porter.stem(w)])\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            res.append(vocab[w])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16206, 62696, 23914]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preproc(\"Очень страшный фильм\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    text, _ = generate_batch([{\n",
    "        \"discr\":text_preproc(text),\n",
    "        \"ganre\":[]\n",
    "    }])\n",
    "    return text\n",
    "\n",
    "test = predict(\"Там, где круглый год лежат снега, а небо озаряет северное сияние, живет Ледяная принцесса Лилли с голубыми, как древний лед, волосами. Однажды Лилли и её друг, полярный медведь Лимбо, встречают юного дракона, который не умеет извергать пламя. Лилли обещает помочь ему. Но путь, который их ждет, будет не из легких, впереди новых друзей ждет немало приключений и опасных испытаний.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.h_0 = torch.zeros(1,1, 128)\n",
    "model.c_0 = torch.zeros(1,1, 128)\n",
    "\n",
    "pred=model(test)[0][0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Романтический\n",
      "Криминал\n",
      "Комедия\n",
      "Триллер\n",
      "Драма\n"
     ]
    }
   ],
   "source": [
    "top = sorted(pred)[-5:]\n",
    "for t in top:\n",
    "    print(gange[pred.index(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGanre(\n",
       "  (embedding): Embedding(69127, 16)\n",
       "  (basic_rnn): LSTM(16, 128)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (clasifiter): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=26, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = torch.nn.Sequential(*(list(model.children())[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(newmodel.state_dict(), \"model_lstm_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
